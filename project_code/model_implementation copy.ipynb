{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pyvis\n",
    "from pyvis.network import Network\n",
    "import networkx as nx\n",
    "from matplotlib import pyplot as plt\n",
    "from itertools import product\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import project_code.utils as ut\n",
    "from project_code.preprocessing_classes import DataExploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_exploration = DataExploration()\n",
    "skus_data, transactions_data = data_exploration.run_data_preprocessing_piepeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skus_data_plot = skus_data[(skus_data[\"DEPARTMENT\"]==\"Services\")]\n",
    "# hierarchical_categories = skus_data_plot[[\"DEPARTMENT\", \"CATEGORY\", \"SUBCATEGORY1\"]].drop_duplicates()\n",
    "# fig = px.treemap(hierarchical_categories, path=[\"DEPARTMENT\", \"CATEGORY\", \"SUBCATEGORY1\"])\n",
    "# fig.write_image(f\"{ut.CURRENT_DIRECTORY}/data/services_department.png\")\n",
    "# fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skus_data_plot = skus_data[(skus_data[\"DEPARTMENT\"]==\"Weighed\")]\n",
    "# hierarchical_categories = skus_data_plot[[\"DEPARTMENT\", \"CATEGORY\"]].drop_duplicates()\n",
    "# fig = px.treemap(hierarchical_categories, path=[\"DEPARTMENT\", \"CATEGORY\"])\n",
    "# fig.write_image(f\"{ut.CURRENT_DIRECTORY}/data/weighed_department.png\")\n",
    "# fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skus_data_plot = skus_data[(skus_data[\"DEPARTMENT\"]==\"Sm.Animal\")]\n",
    "# hierarchical_categories = skus_data_plot[[\"DEPARTMENT\", \"CATEGORY\", \"SUBCATEGORY1\"]].drop_duplicates()\n",
    "# fig = px.treemap(hierarchical_categories, path=[\"DEPARTMENT\", \"CATEGORY\", \"SUBCATEGORY1\"])\n",
    "# fig.write_image(f\"{ut.CURRENT_DIRECTORY}/data/Sm.Animal_department.png\")\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation of a department ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_cluster_id(self, row : pd.Series) -> str:\n",
    "#     \"\"\" \n",
    "#     Create a cluster name for every item given Department, Category, Subcategory1, etc.\n",
    "#     \"\"\"\n",
    "#     department_name = row[\"DEPARTMENT\"]\n",
    "#     category_name = row[\"CATEGORY\"]\n",
    "#     subcategory_1_name = row[\"SUBCATEGORY1\"]\n",
    "\n",
    "#     cluster_name = department_name\n",
    "#     if department_name == \"Weighed\":\n",
    "#         cluster_name = category_name\n",
    "#     if department_name == \"Services\":\n",
    "#         cluster_name = subcategory_1_name\n",
    "#     if cluster_name == \"Bird\" or cluster_name == \"Wildbird\" or cluster_name == \"Dom.Bird\":\n",
    "#         cluster_name = \"Bird\"\n",
    "#     return cluster_name\n",
    "\n",
    "# skus_data[\"CLUSTER_NAME\"] = \"\"\n",
    "# skus_data[\"CLUSTER_NAME\"] = skus_data.apply(lambda x : create_cluster_id(x), axis = 1)\n",
    "# skus_data[\"CLUSTER_NAME\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge SKUs and transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_exploration = DataExploration()\n",
    "# transactions_data = data_exploration.merge_skus_transactions(skus_data, transactions_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning\n",
    "### Remove Misc. department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of transactions department Misc. represents: 0.030647782679359182\n",
      "Percentage of revenue department Misc. represents: 0.0021068021703166276\n"
     ]
    }
   ],
   "source": [
    "def remove_department(df_skus : pd.DataFrame, df_transactions : pd.DataFrame, department_name : str = \"Misc.\") -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Remove a department in the datasets. \n",
    "    \"\"\"\n",
    "    df_transactions[df_transactions[\"DEPARTMENT\"]==department_name][\"REVENUE\"].sum()/df_transactions[\"REVENUE\"].sum()\n",
    "    print(f\"Percentage of transactions department {department_name} represents: {len(df_transactions[df_transactions['DEPARTMENT']==department_name])/len(df_transactions)}\")\n",
    "    print(f\"Percentage of revenue department {department_name} represents: {df_transactions[df_transactions['DEPARTMENT']==department_name]['REVENUE'].sum()/df_transactions['REVENUE'].sum()}\")\n",
    "    df_transactions = df_transactions[df_transactions[\"DEPARTMENT\"]!=department_name]\n",
    "    df_skus = df_skus[df_skus[\"DEPARTMENT\"]!=department_name]\n",
    "    return df_skus, df_transactions\n",
    "skus_data, transactions_data = remove_department(skus_data, transactions_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Small Animals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(transactions_data[transactions_data[\"DEPARTMENT\"]==\"Sm.Animal\"])/len(transactions_data), transactions_data[transactions_data[\"DEPARTMENT\"]==\"Sm.Animal\"][\"REVENUE\"].sum()/transactions_data[\"REVENUE\"].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SKU analysis\n",
    "### Aggregate per sku and count total number of transactions and revenue generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def total_transactions_per_sku(df):\n",
    "#     return df.groupby([\"SKU\"]).agg(\n",
    "#         SKU_TOTAL_TRANSACTION = (\"TRANSACTION_ID\", \"count\"),\n",
    "#         SKU_TOTAL_REVENUE = (\"REVENUE\", \"sum\")\n",
    "#     ).reset_index()\n",
    "\n",
    "# item_purchases = total_transactions_per_sku(transactions_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_transactions_per_cluster(df : pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Return the total number of transactions and revenue per cluster.\n",
    "    \"\"\"\n",
    "    return df.groupby([\"CLUSTER_NAME\"]).agg(\n",
    "        CLUSTER_TOTAL_TRANSACTION = (\"TRANSACTION_ID\", \"count\"),\n",
    "        CLUSTER_TOTAL_REVENUE = (\"REVENUE\", \"sum\")\n",
    "    ).reset_index()\n",
    "\n",
    "cluster_purchases = total_transactions_per_cluster(transactions_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_sku_cluster_statistics(df_skus : pd.DataFrame, df_clusters : pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add clsuter statistics to the SKUs dataset. \n",
    "    \"\"\"\n",
    "    return df_skus.merge(df_clusters, on = \"CLUSTER_NAME\", how = \"left\")\n",
    "\n",
    "skus_data = merge_sku_cluster_statistics(skus_data, cluster_purchases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_support_sku(df : pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate the Support metric of every item.\n",
    "    Support = Total number of transactions Item A / Total number of transactions cluster.\n",
    "    \"\"\"\n",
    "    # df[\"SUPPORT\"] = df[\"SKU_TOTAL_TRANSACTION\"]/df[\"SKU_TOTAL_TRANSACTION\"].sum()\n",
    "    df[\"SUPPORT_CLUSTER\"] = df[\"SKU_TOTAL_TRANSACTION\"]/df[\"CLUSTER_TOTAL_TRANSACTION\"]\n",
    "    return df\n",
    "skus_data = calculate_support_sku(skus_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transactions_data = transactions_data.merge(item_purchases, on = \"SKU\", how = \"left\")\n",
    "# transactions_data = transactions_data.merge(clusters_statistics, on = \"CLUSTER_NAME\", how = \"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count number of times a pair of items is bought together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_all_possible_pairs_sku(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a datasets with all the possible pairs of SKUs.\n",
    "    SKU 1 in clomun 1, SKU 2 in column 2.\n",
    "    Remove pairs with the same SKUs.\n",
    "    \"\"\"\n",
    "    ## Create all possible pairs\n",
    "    pairs = list(product(df[\"SKU\"].sort_values(), repeat=2))\n",
    "\n",
    "    # # Create a new DataFrame with the pairs\n",
    "    pairs_df = pd.DataFrame(pairs, columns=[\"SKU_1\", \"SKU_2\"])\n",
    "    pairs_df[\"TRANSACTION_COUNT\"] = 0\n",
    "\n",
    "    # pairs_df[\"SKU1\"], pairs_df[\"SKU2\"] = zip(*pairs_df.apply(lambda row: sorted([row[\"SKU1\"], row[\"SKU2\"]]), axis=1))\n",
    "    # pairs_df = pairs_df.drop_duplicates()\n",
    "    ## Remove pairs with the same SKUs\n",
    "    pairs_df = pairs_df[pairs_df[\"SKU_1\"] != pairs_df[\"SKU_2\"]]\n",
    "    return pairs_df\n",
    "\n",
    "pairs_df = create_all_possible_pairs_sku(skus_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tuple_skus_sold_together(df : pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Find all the pairs of SKUs sold in the same basket.\n",
    "    \"\"\"\n",
    "    ## Aggregate all the SKUs sold in the same basket into a list\n",
    "    skus_per_transaction = df.groupby([\"TRANSACTION_ID\"]).agg({\n",
    "        \"SKU\" : lambda x : set(list(x))\n",
    "    }).reset_index()\n",
    "\n",
    "    ## Sort the list of SKUs by SKU\n",
    "    skus_per_transaction[\"SKU\"] = skus_per_transaction.SKU.map(lambda x : sorted(x))\n",
    "\n",
    "    ## Create all possible pairs of SKUs for every basket\n",
    "    skus_per_transaction[\"PAIR_SKUS\"] = skus_per_transaction[\"SKU\"].map(lambda x : list(product(x, repeat=2)))\n",
    "    return skus_per_transaction\n",
    "\n",
    "skus_per_transaction = create_tuple_skus_sold_together(transactions_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_pair_purchase(df_pairs_sku, df_transactions_skus):\n",
    "    \"\"\"\n",
    "    Count the number of times a pair of SKUs was sold.\n",
    "    Input:\n",
    "        - Dataframe containing all the possible pairs of SKUs.\n",
    "        - Dataframe containing the SKUs sold together for every transactions.     \n",
    "    Output:\n",
    "        - Dataframe containing all the possible pairs of SKUs.  \n",
    "    \"\"\"\n",
    "    for _, row in df_transactions_skus.iterrows():\n",
    "        list_pairs = row[\"PAIR_SKUS\"]\n",
    "        for _pair in list_pairs:\n",
    "            sku_id1 = _pair[0]\n",
    "            sku_id2 = _pair[1]\n",
    "            # if sku_id1 < sku_id2:\n",
    "            df_pairs_sku.loc[(df_pairs_sku[\"SKU_1\"]==sku_id1) & (df_pairs_sku[\"SKU_2\"]==sku_id2), \"TRANSACTION_COUNT\"] += 1\n",
    "\n",
    "    return df_pairs_sku\n",
    "\n",
    "pairs_df = count_pair_purchase(pairs_df, skus_per_transaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_skus_data(df_pairs : pd.DataFrame, df_skus : pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add statistics about both SKUs contained in every pair of a same basket.\n",
    "    \"\"\"\n",
    "    ## Rename columns of the dataset containing unique SKUs, their cluster and statsitics\n",
    "    skus_data1 = df_skus.copy()\n",
    "    skus_data1.columns = [str(x)+\"_1\" for x in skus_data1.columns]\n",
    "\n",
    "    ## Add statistics for SKU 1 using the suffixe 'SKU_1' \n",
    "    df_pairs = df_pairs.merge(skus_data1, on = \"SKU_1\", how = \"left\")\n",
    "\n",
    "    ## Same for SKU 2\n",
    "    skus_data2 = df_skus.copy()\n",
    "    skus_data2.columns = [str(x)+\"_2\" for x in skus_data2.columns]\n",
    "    df_pairs = df_pairs.merge(skus_data2, on = \"SKU_2\", how = \"left\")\n",
    "\n",
    "    return df_pairs\n",
    "\n",
    "pairs_df = add_skus_data(pairs_df, skus_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairs_df[\"RATIO_1\"] = pairs_df.apply(lambda x : x.TRANSACTION_COUNT / x.SKU_TOTAL_TRANSACTION_1 if x.SKU_TOTAL_TRANSACTION_1 != 0 else 0, axis = 1)\n",
    "# pairs_df[\"RATIO_2\"] = pairs_df.apply(lambda x : x.TRANSACTION_COUNT / x.SKU_TOTAL_TRANSACTION_2 if x.SKU_TOTAL_TRANSACTION_2 != 0 else 0, axis = 1)\n",
    "# pairs_df[\"MAX_RATIO\"] = pairs_df.apply(lambda x : max(x.RATIO_1, x.RATIO_2), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairs_df_filtred = filter_nbre_transactions(pairs_df, 5)\n",
    "# tt = pairs_df_filtred[(pairs_df_filtred[\"MAX_RATIO\"]>=0.05)]\n",
    "# tt = tt[(tt[\"TRANSACTION_COUNT\"]>=2)]\n",
    "\n",
    "# tt[\"SKU_1\"].nunique()\n",
    "\n",
    "# ll = tt.groupby([\"SKU_1\"]).agg(\n",
    "#     nbre_halo_effect = (\"SKU_2\", \"count\")\n",
    "# ).reset_index()\n",
    "\n",
    "# ll[\"nbre_halo_effect\"].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Same cluster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flag_same_cluster(df : pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Flag whether the SKUs contained in a pair have the same cluster.\n",
    "    \"\"\"\n",
    "    df[\"SAME_CLUSTER\"] = df.apply(lambda x : 1 if x.CLUSTER_NAME_1 == x.CLUSTER_NAME_2 else 0, axis = 1)\n",
    "    return df\n",
    "\n",
    "pairs_df = flag_same_cluster(pairs_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apriori algorithm implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features(df : pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform feature engineering and return for every pairs:\n",
    "    - Confidence metric\n",
    "    - Lift metric\n",
    "    - Conviction  metric\n",
    "    \"\"\"\n",
    "    df[\"CONFIDENCE\"] = df[\"TRANSACTION_COUNT\"]/df[\"SKU_TOTAL_TRANSACTION_1\"]\n",
    "    df[\"LIFT\"] = df[\"CONFIDENCE\"]/df[\"SUPPORT_CLUSTER_2\"]\n",
    "    df[\"CONVICTION\"] = (1-df[\"SUPPORT_CLUSTER_2\"])/(1-df[\"CONFIDENCE\"])\n",
    "    return df\n",
    "\n",
    "pairs_df = generate_features(pairs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_nbre_transactions(df : pd.DataFrame, threshold_transactions : pd.DataFrame) -> pd.DataFrame:\n",
    "   \"\"\"\n",
    "   Apply a threshold on the minimum number of transactions over the past 3 months for every SKU.  \n",
    "   \"\"\"\n",
    "   df = df[df[\"SKU_TOTAL_TRANSACTION_1\"]>=threshold_transactions]\n",
    "   df = df[df[\"SKU_TOTAL_TRANSACTION_2\"]>=threshold_transactions]\n",
    "   return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_thresholds = {\n",
    "    \"threshold_number_transactions\" : 5,\n",
    "    \"confidence_threshold\" : 0.1,\n",
    "    \"lift_threshold\" : 1.1,\n",
    "    \"support_pair_threshold\" : 2,\n",
    "    \"conviction_threshold\" : 1.10\n",
    "    }\n",
    "\n",
    "def correlation_skus_detection(df : pd.DataFrame, correlation_thresholds : dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply filters to flag items whose sales are positively correlated (i.e.: measure of halo effect between the two).\n",
    "    \"\"\"\n",
    "    df_filtered = filter_nbre_transactions(df, correlation_thresholds[\"threshold_number_transactions\"])\n",
    "    ## Apply filtering using lift metric\n",
    "    df_filtered = df_filtered[(df_filtered[\"LIFT\"]>=correlation_thresholds[\"lift_threshold\"])]\n",
    "\n",
    "    ## Apply filtering using confidence metric\n",
    "    df_filtered = df_filtered[(df_filtered[\"CONFIDENCE\"]>=correlation_thresholds[\"confidence_threshold\"])]\n",
    "\n",
    "    ## Apply filtering using support metric for the pair of items\n",
    "    df_filtered = df_filtered[(df_filtered[\"TRANSACTION_COUNT\"]>=correlation_thresholds[\"support_pair_threshold\"])]\n",
    "\n",
    "    ## Apply filtering using conviction metric\n",
    "    df_filtered = df_filtered[(df_filtered[\"CONVICTION\"]>=correlation_thresholds[\"conviction_threshold\"])]\n",
    "\n",
    "    ## Only keep pairs from the same cluster\n",
    "    df_filtered = df_filtered[(df_filtered[\"SAME_CLUSTER\"]==1)]\n",
    "    return df_filtered\n",
    "pairs_df_filtred = correlation_skus_detection(pairs_df, correlation_thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(0.40018135524222903), 170)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pairs_df_filtred[\"SAME_CLUSTER\"].value_counts()\n",
    "skus_data[skus_data[\"SKU\"].isin(pairs_df_filtred[\"SKU_1\"].unique())][\"SKU_TOTAL_REVENUE\"].sum()/skus_data[\"SKU_TOTAL_REVENUE\"].sum(), pairs_df_filtred[\"SKU_1\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SKU_2\n",
       "1    71\n",
       "3    53\n",
       "2    46\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_top_3_upsells(df : pd.DataFrame) -> pd.DataFrame:\n",
    " \"\"\"\n",
    " For every item find the top 3 most correlated other items.\n",
    " \"\"\"\n",
    " return df.sort_values(by=[\"SKU_1\", \"LIFT\"], ascending=[True, False]).groupby(\"SKU_1\").head(3).reset_index(drop=True) \n",
    "\n",
    "pairs_df_top3 = find_top_3_upsells(pairs_df_filtred)   \n",
    "pairs_df_top3.groupby(\"SKU_1\").agg(\n",
    "    {\"SKU_2\" : \"count\"}\n",
    ").reset_index().value_counts(\"SKU_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Halo effect degree n+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest_neighbors_node(G) -> dict:\n",
    "    \"\"\"\n",
    "    Find the nearest neighbors of all the nodes in the graph and the weight of the edge between the node and its neighbors\n",
    "    with a degree of 1. \n",
    "    \"\"\"\n",
    "    ## Create a dictionnary with all the nodes of the graph as keys\n",
    "    node_neighbor_weights = {_node: {}  for _node in G.nodes()}\n",
    "\n",
    "    ## For every node of the graph:\n",
    "    for _node in G.nodes():\n",
    "        ## Find the nearest neighbors\n",
    "        node_neighbors = G.neighbors(_node)    \n",
    "        for _neighbor in node_neighbors:\n",
    "            ## Return the value of the weight for the edge between the node and its neighbor\n",
    "            edge_neighbor = G.get_edge_data(_node, _neighbor)\n",
    "            \n",
    "            ## Add the neighbor ID and the edge value to the dictionnary\n",
    "            node_neighbor_weights[int(_node)][int(_neighbor)] = edge_neighbor[\"weight\"]\n",
    "    return node_neighbor_weights\n",
    "\n",
    "def order_neighbors_per_weight(node_neighbors_dict : dict) -> dict:\n",
    "    \"\"\"\n",
    "    For every node, it orders the list of its nearest neighbors given the weight of the edge between them. \n",
    "    \"\"\"\n",
    "    for _node, _neighbors in node_neighbors_dict.items():\n",
    "        node_neighbors_dict[_node] = dict(sorted(_neighbors.items(), key=lambda item: item[1], reverse = True))\n",
    "    return node_neighbors_dict\n",
    "\n",
    "def find_n_plus_1_neighbors_node(G, node_neighbors_dict : dict) -> dict:\n",
    "    \"\"\"\n",
    "    For every node, it goes through the list of its nearest neighbors ordered by importance (i.e.: the weight of the edge).\n",
    "    It then finds their nearest neighbors (i.e.: they are the neighbors of the original node with a degree of 2).\n",
    "    It orders the list of neighbors given their weight.\n",
    "    It returns the original list of nodes with their neighbors with a degree of 2 ordered by their weight.\n",
    "    \"\"\"\n",
    "    ## Create a dictionnary with all the nodes of the graph as keys\n",
    "    node_neighbor_weights_n_1 = {_node: []  for _node in G.nodes()}\n",
    "\n",
    "    ## Iter through all the nearest neighbors of every nodes\n",
    "    for _node_0, _neighbors_1 in node_neighbors_dict.items():\n",
    "        ## Create a dictionnary to add all the nodes with a of degree 2\n",
    "        node_neighbor_weights_n_2 = {_node: {}  for _node in _neighbors_1.keys()}\n",
    "        for _neighbor_1 in _neighbors_1.keys():\n",
    "            ## Find the neighbors (degree 2) of the neighbor (degree 1)\n",
    "            node_neighbors_2 = G.neighbors(_neighbor_1)    \n",
    "            for _neighbor_2 in node_neighbors_2:\n",
    "                ## Save the neighbor with a degree of 2 and the weight of the edge shared with neighbor with a degree of 1\n",
    "                edge_neighbor = G.get_edge_data(_neighbor_1, _neighbor_2)\n",
    "                node_neighbor_weights_n_2[int(_neighbor_1)][int(_neighbor_2)] = edge_neighbor[\"weight\"]\n",
    "\n",
    "            node_neighbor_weights_n_2 = order_neighbors_per_weight(node_neighbor_weights_n_2)\n",
    "\n",
    "            node_neighbor_weights_n_1[_node_0] += list(node_neighbor_weights_n_2[_neighbor_1].keys())\n",
    "    return node_neighbor_weights_n_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "\n",
    "for pos, row in pairs_df_top3.iterrows():\n",
    "    node1 = row[\"SKU_1\"]\n",
    "    node2 = row[\"SKU_2\"]\n",
    "\n",
    "    G.add_node(int(row[\"SKU_1\"]), label=row[\"ITEM_DESCRIPTION_1\"])\n",
    "    G.add_node(int(row[\"SKU_2\"]), label=row[\"ITEM_DESCRIPTION_2\"])\n",
    "    G.add_edge(int(row[\"SKU_1\"]), int(row[\"SKU_2\"]), weight=row[\"CONVICTION\"])\n",
    "\n",
    "def remove_node_neighbors(node_neighbors_dict : dict) -> dict:\n",
    "    \"\"\"\n",
    "    Remove the node ID of its neighbors with a degree of 2 list.\n",
    "    \"\"\"\n",
    "    for _node, _neighbors in node_neighbors_dict.items():\n",
    "        node_neighbors_dict[_node] = [x for x in _neighbors if x!=_node]\n",
    "    return node_neighbors_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_neighbor_weights = find_nearest_neighbors_node(G)\n",
    "\n",
    "node_neighbor_weights_order = order_neighbors_per_weight(node_neighbor_weights)\n",
    "\n",
    "node_neighbor_n_plus_one = find_n_plus_1_neighbors_node(G, node_neighbor_weights_order)\n",
    "\n",
    "node_neighbor_n_plus_one = remove_node_neighbors(node_neighbor_n_plus_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_possible_neighbors() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a sparse dataset for every possible pair of SKUs.\n",
    "    NEIGHBOR_RANKING is a score given to SKU 2 - neighbor of SKU 1 with a degree of 2.\n",
    "    NEIGHBOR_RANKING = 1, 2 or 3, with 1 beign the best. \n",
    "    \"\"\"\n",
    "    df = create_all_possible_pairs_sku(skus_data)\n",
    "    df[\"NEIGHBOR_RANKING\"] = 0\n",
    "    return df\n",
    "\n",
    "neighbors_degree_2 = all_possible_neighbors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicated_neighbors(node_neighbors_dict : dict) -> dict:\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        - node_neighbor_n_plus_one: Key: SKU, value: list of every neighbors with a degree of 2.\n",
    "    Remove duplicated neighbors for every SKU.\n",
    "    \"\"\"\n",
    "    for _node, _neighbors in node_neighbors_dict.items():\n",
    "        node_neighbors_dict[_node] = list(dict.fromkeys(_neighbors))\n",
    "    return node_neighbors_dict\n",
    "\n",
    "node_neighbor_n_plus_one = remove_duplicated_neighbors(node_neighbor_n_plus_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_neighbors_n_2(node_neighbors_dict : dict, neighbors_degree_2 : pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        - node_neighbor_n_plus_one: Key: SKU, value: list of every neighbors with a degree of 2.\n",
    "    For every SKU, return the 3 best neighbors with a degree of 2, ranked from 1 to 3.\n",
    "    \"\"\"\n",
    "    for _node, _neighbors in node_neighbors_dict.items():\n",
    "        count_iter = 1\n",
    "        for node_neighbor in _neighbors:\n",
    "                if count_iter < 4:\n",
    "                    neighbors_degree_2.loc[(neighbors_degree_2[\"SKU_1\"]==_node) & (neighbors_degree_2[\"SKU_2\"]==node_neighbor), \"TRANSACTION_COUNT\"] = 1\n",
    "                    neighbors_degree_2.loc[(neighbors_degree_2[\"SKU_1\"]==_node) & (neighbors_degree_2[\"SKU_2\"]==node_neighbor), \"NEIGHBOR_RANKING\"] = count_iter\n",
    "                    count_iter +=1\n",
    "    return neighbors_degree_2\n",
    "\n",
    "neighbors_degree_2 = find_best_neighbors_n_2(node_neighbor_n_plus_one, neighbors_degree_2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SKU_1</th>\n",
       "      <th>SKU_2</th>\n",
       "      <th>TRANSACTION_COUNT</th>\n",
       "      <th>NEIGHBOR_RANKING</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2464</th>\n",
       "      <td>9518</td>\n",
       "      <td>9517</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2614</th>\n",
       "      <td>9518</td>\n",
       "      <td>45806</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2823</th>\n",
       "      <td>9518</td>\n",
       "      <td>56439</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      SKU_1  SKU_2  TRANSACTION_COUNT  NEIGHBOR_RANKING\n",
       "2464   9518   9517                  1                 1\n",
       "2614   9518  45806                  1                 3\n",
       "2823   9518  56439                  1                 2"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neighbors_degree_2 = neighbors_degree_2[neighbors_degree_2[\"TRANSACTION_COUNT\"]>0]\n",
    "neighbors_degree_2[(neighbors_degree_2[\"SKU_1\"]==9518) & (neighbors_degree_2[\"SKU_2\"].isin(node_neighbor_n_plus_one[9518]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors_degree_2 = add_skus_data(neighbors_degree_2, skus_data) \n",
    "neighbors_degree_2 = generate_features(neighbors_degree_2)\n",
    "neighbors_degree_2 = flag_same_cluster(neighbors_degree_2)\n",
    "\n",
    "pairs_df_top3[\"NEIGHBOR_RANKING\"] = 0\n",
    "\n",
    "pairs_df_top_neighbors = pd.concat([pairs_df_top3, neighbors_degree_2])\n",
    "pairs_df_top_neighbors = pairs_df_top_neighbors.sort_values(by=[\"SKU_1\", \"NEIGHBOR_RANKING\"], ascending=[True, True]).groupby(\"SKU_1\").head(3).reset_index(drop=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SKU_2\n",
       "3    123\n",
       "1     29\n",
       "2     26\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs_df_top_neighbors.groupby(\"SKU_1\").agg(\n",
    "    {\"SKU_2\" : \"count\"}\n",
    ").reset_index().value_counts(\"SKU_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/guillaume/jollyes')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Get the current working directory\n",
    "cwd = Path.cwd()\n",
    "cwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "teams.html\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"600px\"\n",
       "            src=\"teams.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f2e79d3acb0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Network(notebook=True, filter_menu=True, cdn_resources='remote')\n",
    "for edge, data in G.edges.items():\n",
    "    data[\"label\"] = str(data[\"weight\"])\n",
    "    \n",
    "net.from_nx(G)\n",
    "net.show_buttons(filter_=['physics'])\n",
    "net.show_buttons()\n",
    "net.show(\"teams.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
