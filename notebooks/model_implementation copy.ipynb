{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph_objects\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgo\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexpress\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpx\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mut\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mproject_code\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_processing_classes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataExploration\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pyvis\n",
    "from pyvis.network import Network\n",
    "import networkx as nx\n",
    "from matplotlib import pyplot as plt\n",
    "from itertools import product\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import utils as ut\n",
    "from project_code.data_processing_classes import DataExploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_exploration = DataExploration()\n",
    "skus_data, transactions_data = data_exploration.run_data_preprocessing_piepeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- # Creation of a department ID -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- # Merge SKUs and transactions -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- # Data cleaning\n",
    "### Remove Misc. department -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AprioriModel(DataExploration):\n",
    "    @staticmethod\n",
    "    def filter_nbre_transactions(df : pd.DataFrame, threshold_transactions : pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Apply a threshold on the minimum number of transactions over the past 3 months for every SKU.  \n",
    "        \"\"\"\n",
    "        df = df[df[\"SKU_TOTAL_TRANSACTION_1\"]>=threshold_transactions]\n",
    "        df = df[df[\"SKU_TOTAL_TRANSACTION_2\"]>=threshold_transactions]\n",
    "        return df\n",
    "    \n",
    "    def create_all_possible_pairs_sku(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create a datasets with all the possible pairs of SKUs.\n",
    "        SKU 1 in clomun 1, SKU 2 in column 2.\n",
    "        Remove pairs with the same SKUs.\n",
    "        \"\"\"\n",
    "        ## Create all possible pairs\n",
    "        pairs = list(product(df[\"SKU\"].sort_values(), repeat=2))\n",
    "\n",
    "        # # Create a new DataFrame with the pairs\n",
    "        pairs_df = pd.DataFrame(pairs, columns=[\"SKU_1\", \"SKU_2\"])\n",
    "        pairs_df[\"TRANSACTION_COUNT\"] = 0\n",
    "        \n",
    "        ## Remove pairs with the same SKUs\n",
    "        pairs_df = pairs_df[pairs_df[\"SKU_1\"] != pairs_df[\"SKU_2\"]]\n",
    "        return pairs_df\n",
    "    \n",
    "    def create_tuple_skus_sold_together(self, df : pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Find all the pairs of SKUs sold in the same basket.\n",
    "        \"\"\"\n",
    "        ## Aggregate all the SKUs sold in the same basket into a list\n",
    "        skus_per_transaction = df.groupby([\"TRANSACTION_ID\"]).agg({\n",
    "            \"SKU\" : lambda x : set(list(x))\n",
    "        }).reset_index()\n",
    "\n",
    "        ## Sort the list of SKUs by SKU\n",
    "        skus_per_transaction[\"SKU\"] = skus_per_transaction.SKU.map(lambda x : sorted(x))\n",
    "\n",
    "        ## Create all possible pairs of SKUs for every basket\n",
    "        skus_per_transaction[\"PAIR_SKUS\"] = skus_per_transaction[\"SKU\"].map(lambda x : list(product(x, repeat=2)))\n",
    "        return skus_per_transaction\n",
    "    \n",
    "    def count_pair_purchase(self, df_pairs_sku : pd.DataFrame, df_transactions_skus : pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Count the number of times a pair of SKUs was sold together.\n",
    "        Input:\n",
    "            - Dataframe containing all the possible pairs of SKUs.\n",
    "            - Dataframe containing the SKUs sold together for every transactions.     \n",
    "        Output:\n",
    "            - Dataframe containing all the possible pairs of SKUs.  \n",
    "        \"\"\"\n",
    "        for _, row in df_transactions_skus.iterrows():\n",
    "            list_pairs = row[\"PAIR_SKUS\"]\n",
    "            for _pair in list_pairs:\n",
    "                sku_id1 = _pair[0]\n",
    "                sku_id2 = _pair[1]\n",
    "                # if sku_id1 < sku_id2:\n",
    "                df_pairs_sku.loc[(df_pairs_sku[\"SKU_1\"]==sku_id1) & (df_pairs_sku[\"SKU_2\"]==sku_id2), \"TRANSACTION_COUNT\"] += 1\n",
    "\n",
    "        return df_pairs_sku\n",
    "    \n",
    "    def add_skus_data(self, df_pairs : pd.DataFrame, df_skus : pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Add statistics about both SKUs contained in every pair of a same basket.\n",
    "        \"\"\"\n",
    "        ## Rename columns of the dataset containing unique SKUs, their cluster and statsitics\n",
    "        skus_data1 = df_skus.copy()\n",
    "        skus_data1.columns = [str(x)+\"_1\" for x in skus_data1.columns]\n",
    "\n",
    "        ## Add statistics for SKU 1 using the suffixe 'SKU_1' \n",
    "        df_pairs = df_pairs.merge(skus_data1, on = \"SKU_1\", how = \"left\")\n",
    "\n",
    "        ## Same for SKU 2\n",
    "        skus_data2 = df_skus.copy()\n",
    "        skus_data2.columns = [str(x)+\"_2\" for x in skus_data2.columns]\n",
    "        df_pairs = df_pairs.merge(skus_data2, on = \"SKU_2\", how = \"left\")\n",
    "\n",
    "        return df_pairs\n",
    "    \n",
    "    def flag_same_cluster(self, df : pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Flag whether the SKUs contained in a pair have the same cluster.\n",
    "        \"\"\"\n",
    "        df[\"SAME_CLUSTER\"] = df.apply(lambda x : 1 if x.CLUSTER_NAME_1 == x.CLUSTER_NAME_2 else 0, axis = 1)\n",
    "        return df\n",
    "    \n",
    "    def generate_features(self, df : pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Perform feature engineering and return for every pairs:\n",
    "        - Confidence metric\n",
    "        - Lift metric\n",
    "        - Conviction  metric\n",
    "        \"\"\"\n",
    "        df[\"CONFIDENCE\"] = df[\"TRANSACTION_COUNT\"]/df[\"SKU_TOTAL_TRANSACTION_1\"]\n",
    "        df[\"LIFT\"] = df[\"CONFIDENCE\"]/df[\"SUPPORT_CLUSTER_2\"]\n",
    "        df[\"CONVICTION\"] = (1-df[\"SUPPORT_CLUSTER_2\"])/(1-df[\"CONFIDENCE\"])\n",
    "        return df\n",
    "    \n",
    "\n",
    "    def correlation_skus_detection(self, df : pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Apply filters to flag items whose sales are positively correlated (i.e.: measure of halo effect between the two).\n",
    "        \"\"\"\n",
    "        df_filtered = AprioriModel.filter_nbre_transactions(df, ut.CORRELATION_THRESHOLDS[\"threshold_number_transactions\"])\n",
    "        ## Apply filtering using lift metric\n",
    "        df_filtered = df_filtered[(df_filtered[\"LIFT\"]>=ut.CORRELATION_THRESHOLDS[\"lift_threshold\"])]\n",
    "\n",
    "        ## Apply filtering using confidence metric\n",
    "        df_filtered = df_filtered[(df_filtered[\"CONFIDENCE\"]>=ut.CORRELATION_THRESHOLDS[\"confidence_threshold\"])]\n",
    "\n",
    "        ## Apply filtering using support metric for the pair of items\n",
    "        df_filtered = df_filtered[(df_filtered[\"TRANSACTION_COUNT\"]>=ut.CORRELATION_THRESHOLDS[\"support_pair_threshold\"])]\n",
    "\n",
    "        ## Apply filtering using conviction metric\n",
    "        df_filtered = df_filtered[(df_filtered[\"CONVICTION\"]>=ut.CORRELATION_THRESHOLDS[\"conviction_threshold\"])]\n",
    "\n",
    "        ## Only keep pairs from the same cluster\n",
    "        df_filtered = df_filtered[(df_filtered[\"SAME_CLUSTER\"]==1)]\n",
    "        return df_filtered\n",
    "    \n",
    "    def find_top_3_upsells(self, df : pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        For every item find the top 3 most correlated other items.\n",
    "        \"\"\"\n",
    "        return df.sort_values(by=[\"SKU_1\", \"LIFT\"], ascending=[True, False]).groupby(\"SKU_1\").head(3).reset_index(drop=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- # SKU analysis\n",
    "### Aggregate per sku and count total number of transactions and revenue generated -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def total_transactions_per_sku(df):\n",
    "#     return df.groupby([\"SKU\"]).agg(\n",
    "#         SKU_TOTAL_TRANSACTION = (\"TRANSACTION_ID\", \"count\"),\n",
    "#         SKU_TOTAL_REVENUE = (\"REVENUE\", \"sum\")\n",
    "#     ).reset_index()\n",
    "\n",
    "# item_purchases = total_transactions_per_sku(transactions_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_model = AprioriModel()\n",
    "## Generate revenue and transactions statistics per cluster\n",
    "cluster_purchases = build_model.total_transactions_per_cluster(transactions_data)\n",
    "\n",
    "## Merge SKUs and Clusters datasets\n",
    "skus_data = build_model.merge_sku_cluster_statistics(skus_data, cluster_purchases)\n",
    "\n",
    "## Calculate the Support metric of every item givven the total number of transactions per cluster\n",
    "skus_data = build_model.calculate_support_sku(skus_data)\n",
    "\n",
    "##Create all possible pairs of SKUs\n",
    "pairs_df = build_model.create_all_possible_pairs_sku(skus_data)\n",
    "\n",
    "## Find all the pairs of SKUs sold in the same basket\n",
    "skus_per_transaction = build_model.create_tuple_skus_sold_together(transactions_data)\n",
    "\n",
    "## Count the number of times a pair of SKUs was sold together\n",
    "pairs_df = build_model.count_pair_purchase(pairs_df, skus_per_transaction)\n",
    "\n",
    "## Merge SKUs and pairs of SKUs datasets\n",
    "pairs_df = build_model.add_skus_data(pairs_df, skus_data)\n",
    "\n",
    "## Flag whether a pair of SKUs is from the same cluster\n",
    "pairs_df = build_model.flag_same_cluster(pairs_df)\n",
    "\n",
    "## Generate metrics (Lift, Confidence etc.)\n",
    "pairs_df = build_model.generate_features(pairs_df)\n",
    "\n",
    "## Apply thresholds to the metrics to flag correlated items\n",
    "pairs_df_filtred = build_model.correlation_skus_detection(pairs_df)\n",
    "\n",
    "## Return 3 items to up-sell per unique SKU\n",
    "pairs_df_top3 = build_model.find_top_3_upsells(pairs_df_filtred)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- # Graph approach -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeighborsGraph(AprioriModel):\n",
    "    def find_nearest_neighbors_node(self, G) -> dict:\n",
    "        \"\"\"\n",
    "        Find the nearest neighbors of all the nodes in the graph and the weight of the edge between the node and its neighbors\n",
    "        with a degree of 1. \n",
    "        \"\"\"\n",
    "        ## Create a dictionnary with all the nodes of the graph as keys\n",
    "        node_neighbor_weights = {_node: {}  for _node in G.nodes()}\n",
    "\n",
    "        ## For every node of the graph:\n",
    "        for _node in G.nodes():\n",
    "            ## Find the nearest neighbors\n",
    "            node_neighbors = G.neighbors(_node)    \n",
    "            for _neighbor in node_neighbors:\n",
    "                ## Return the value of the weight for the edge between the node and its neighbor\n",
    "                edge_neighbor = G.get_edge_data(_node, _neighbor)\n",
    "                \n",
    "                ## Add the neighbor ID and the edge value to the dictionnary\n",
    "                node_neighbor_weights[int(_node)][int(_neighbor)] = edge_neighbor[\"weight\"]\n",
    "        return node_neighbor_weights\n",
    "\n",
    "    def order_neighbors_per_weight(self, node_neighbors_dict : dict) -> dict:\n",
    "            \"\"\"\n",
    "            For every node, it orders the list of its nearest neighbors given the weight of the edge between them. \n",
    "            \"\"\"\n",
    "            for _node, _neighbors in node_neighbors_dict.items():\n",
    "                node_neighbors_dict[_node] = dict(sorted(_neighbors.items(), key=lambda item: item[1], reverse = True))\n",
    "            return node_neighbors_dict\n",
    "\n",
    "    def find_n_plus_1_neighbors_node(self, G, node_neighbors_dict : dict) -> dict:\n",
    "        \"\"\"\n",
    "        For every node, it goes through the list of its nearest neighbors ordered by importance (i.e.: the weight of the edge).\n",
    "        It then finds their nearest neighbors (i.e.: they are the neighbors of the original node with a degree of 2).\n",
    "        It orders the list of neighbors given their weight.\n",
    "        It returns the original list of nodes with their neighbors with a degree of 2 ordered by their weight.\n",
    "        \"\"\"\n",
    "        ## Create a dictionnary with all the nodes of the graph as keys\n",
    "        node_neighbor_weights_n_1 = {_node: []  for _node in G.nodes()}\n",
    "\n",
    "        ## Iter through all the nearest neighbors of every nodes\n",
    "        for _node_0, _neighbors_1 in node_neighbors_dict.items():\n",
    "            ## Create a dictionnary to add all the nodes with a of degree 2\n",
    "            node_neighbor_weights_n_2 = {_node: {}  for _node in _neighbors_1.keys()}\n",
    "            for _neighbor_1 in _neighbors_1.keys():\n",
    "                ## Find the neighbors (degree 2) of the neighbor (degree 1)\n",
    "                node_neighbors_2 = G.neighbors(_neighbor_1)    \n",
    "                for _neighbor_2 in node_neighbors_2:\n",
    "                    ## Save the neighbor with a degree of 2 and the weight of the edge shared with neighbor with a degree of 1\n",
    "                    edge_neighbor = G.get_edge_data(_neighbor_1, _neighbor_2)\n",
    "                    node_neighbor_weights_n_2[int(_neighbor_1)][int(_neighbor_2)] = edge_neighbor[\"weight\"]\n",
    "\n",
    "                node_neighbor_weights_n_2 = self.order_neighbors_per_weight(node_neighbor_weights_n_2)\n",
    "\n",
    "                node_neighbor_weights_n_1[_node_0] += list(node_neighbor_weights_n_2[_neighbor_1].keys())\n",
    "        return node_neighbor_weights_n_1\n",
    "\n",
    "    def create_graph(self, df : pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Create a graph to represent the intereactions between all the SKUs.\n",
    "        - Node: a unique SKU\n",
    "        - Edge: the conviction score between Node 1 and Node 2 \n",
    "        \"\"\"\n",
    "        G = nx.Graph()\n",
    "        for _, row in df.iterrows():\n",
    "            G.add_node(int(row[\"SKU_1\"]), label=row[\"ITEM_DESCRIPTION_1\"])\n",
    "            G.add_node(int(row[\"SKU_2\"]), label=row[\"ITEM_DESCRIPTION_2\"])\n",
    "            G.add_edge(int(row[\"SKU_1\"]), int(row[\"SKU_2\"]), weight=row[\"CONVICTION\"])\n",
    "        return G\n",
    "    \n",
    "    def remove_node_neighbors(self, node_neighbors_dict : dict) -> dict:\n",
    "        \"\"\"\n",
    "        Remove the node ID of its neighbors with a degree of 2 list.\n",
    "        \"\"\"\n",
    "        for _node, _neighbors in node_neighbors_dict.items():\n",
    "            node_neighbors_dict[_node] = [x for x in _neighbors if x!=_node]\n",
    "        return node_neighbors_dict\n",
    "    \n",
    "    def all_possible_neighbors(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create a sparse dataset for every possible pair of SKUs.\n",
    "        NEIGHBOR_RANKING is a score given to SKU 2 - neighbor of SKU 1 with a degree of 2.\n",
    "        NEIGHBOR_RANKING = 1, 2 or 3, with 1 beign the best. \n",
    "        \"\"\"\n",
    "        df = self.create_all_possible_pairs_sku(skus_data)\n",
    "        df[\"NEIGHBOR_RANKING\"] = 0\n",
    "        return df\n",
    "    \n",
    "    def remove_duplicated_neighbors(self, node_neighbors_dict : dict) -> dict:\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            - node_neighbor_n_plus_one: Key: SKU, value: list of every neighbors with a degree of 2.\n",
    "        Remove duplicated neighbors for every SKU.\n",
    "        \"\"\"\n",
    "        for _node, _neighbors in node_neighbors_dict.items():\n",
    "            node_neighbors_dict[_node] = list(dict.fromkeys(_neighbors))\n",
    "        return node_neighbors_dict\n",
    "    \n",
    "    def find_best_neighbors_n_2(self, node_neighbors_dict : dict, neighbors_degree_2 : pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            - node_neighbor_n_plus_one: Key: SKU, value: list of every neighbors with a degree of 2.\n",
    "        For every SKU, return the 3 best neighbors with a degree of 2, ranked from 1 to 3.\n",
    "        \"\"\"\n",
    "        for _node, _neighbors in node_neighbors_dict.items():\n",
    "            count_iter = 1\n",
    "            for node_neighbor in _neighbors:\n",
    "                    if count_iter < 4:\n",
    "                        neighbors_degree_2.loc[(neighbors_degree_2[\"SKU_1\"]==_node) & (neighbors_degree_2[\"SKU_2\"]==node_neighbor), \"TRANSACTION_COUNT\"] = 1\n",
    "                        neighbors_degree_2.loc[(neighbors_degree_2[\"SKU_1\"]==_node) & (neighbors_degree_2[\"SKU_2\"]==node_neighbor), \"NEIGHBOR_RANKING\"] = count_iter\n",
    "                        count_iter +=1\n",
    "        return neighbors_degree_2[neighbors_degree_2[\"TRANSACTION_COUNT\"]>0]\n",
    "    \n",
    "    def add_features_to_neighbors_n_2(self, df : pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Calculate features (lift, confidence etc.) for the new neighbors with a degree of 2.\n",
    "        Flag the pairs with from the same cluster.\n",
    "        \"\"\"\n",
    "        df = self.add_skus_data(df, skus_data) \n",
    "        df = self.generate_features(df)\n",
    "        df = self.flag_same_cluster(df)\n",
    "        return df\n",
    "    \n",
    "    \n",
    "    def add_neighbors_n_2_to_n_1(self, df_pairs : pd.DataFrame, df_neighbors_n_2 : pd.DataFrame) -> pd.DataFrame:\n",
    "            \"\"\"\n",
    "            Concatenate neighbors with a degree of 1 and a degree of 3.\n",
    "            For every SKU, returns the closest:\n",
    "                - Keep the N neighbors at a degree of 1 which made the cut\n",
    "                - If N<3, keep the neighbors with a degree of 2 so that the maximum number of up-sell items per unique SKU is 3\n",
    "            \"\"\"\n",
    "            df_pairs[\"NEIGHBOR_RANKING\"] = 0\n",
    "            pairs_df_top_neighbors = pd.concat([df_pairs, df_neighbors_n_2])\n",
    "            return pairs_df_top_neighbors.sort_values(by=[\"SKU_1\", \"NEIGHBOR_RANKING\"], ascending=[True, True]).groupby(\"SKU_1\").head(3).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- # Halo effect degree n+1 -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors_graph = NeighborsGraph()\n",
    "## Create graph using pairs of SKUs and metrics\n",
    "G = neighbors_graph.create_graph(pairs_df_top3)\n",
    "\n",
    "## Find the nearest neighbor (with a degree of 1) for every SKU\n",
    "node_neighbor_weights = neighbors_graph.find_nearest_neighbors_node(G)\n",
    "\n",
    "## Order every nearest neighbour given a metric\n",
    "node_neighbor_weights_order = neighbors_graph.order_neighbors_per_weight(node_neighbor_weights)\n",
    "\n",
    "## Find the neighbors with a degree of 2 for every SKU\n",
    "node_neighbor_n_plus_one = neighbors_graph.find_n_plus_1_neighbors_node(G, node_neighbor_weights_order)\n",
    "\n",
    "## Remove the node from the lost of its neighbors\n",
    "node_neighbor_n_plus_one = neighbors_graph.remove_node_neighbors(node_neighbor_n_plus_one)\n",
    "\n",
    "## Remove duplicated in the list of neighbors\n",
    "node_neighbor_n_plus_one = neighbors_graph.remove_duplicated_neighbors(node_neighbor_n_plus_one)\n",
    "\n",
    "## Create a sparse dataset with all the SKU pairs possible\n",
    "neighbors_degree_2 = neighbors_graph.all_possible_neighbors()\n",
    "\n",
    "## Find the best 3 neighbors with a degree of 2\n",
    "neighbors_degree_2 = neighbors_graph.find_best_neighbors_n_2(node_neighbor_n_plus_one, neighbors_degree_2)\n",
    "\n",
    "## Calculate the metrics between a node and its neighbors with a degree of 2\n",
    "neighbors_degree_2 = neighbors_graph.add_features_to_neighbors_n_2(neighbors_degree_2)\n",
    "\n",
    "## Add neighbors with a degree of 2 to the neighbors with a degree of 1 (existing top 3 up-sell items for every SKU)\n",
    "## Fill the top 3 with neighbors with a degree of 2 if top 3 incomplete\n",
    "pairs_df_top_neighbors = neighbors_graph.add_neighbors_n_2_to_n_1(pairs_df_top3, neighbors_degree_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_upsell_reco(df : pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Format the dataframe to generate a file containing SKUs with the other items to up-sell. \n",
    "    \"\"\"\n",
    "    ## Order the items so that 'rec 1' is better than 'rec 2', better than 'rec 3'\n",
    "    pairs_df_top_neighbors_sorted = df.sort_values(by=[\"SKU_1\", \"NEIGHBOR_RANKING\", \"CONVICTION\"], ascending=[True, True, False])\n",
    "\n",
    "    ## Group by SKU 1 and collect sorted SKU 2 as a list\n",
    "    pairs_df_top_neighbors_sorted = pairs_df_top_neighbors_sorted.groupby(\"SKU_1\")[\"SKU_2\"].apply(list).reset_index()\n",
    "\n",
    "    ## Create a column for every up-sell item\n",
    "    df_reco = pairs_df_top_neighbors_sorted.join(\n",
    "        pd.DataFrame(pairs_df_top_neighbors_sorted[\"SKU_2\"].tolist(), index=pairs_df_top_neighbors_sorted.index, columns=[\"rec 1\", \"rec 2\", \"rec 3\"])\n",
    "    )\n",
    "\n",
    "    ## Drop and rename columns before saving file\n",
    "    df_expanded = df_reco.drop(columns=[\"SKU_2\"])\n",
    "    # df_expanded = df_expanded.fillna(0)\n",
    "    # df_expanded = df_expanded.astype(int)\n",
    "\n",
    "    df_expanded = df_expanded.rename(columns = {\"SKU_1\" : \"sku\"})\n",
    "\n",
    "    ## Save file\n",
    "    df_expanded.to_csv(f\"{ut.CURRENT_DIRECTORY}/results/upsell_results.csv\", index=False)\n",
    "    return df_expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sku</th>\n",
       "      <th>rec 1</th>\n",
       "      <th>rec 2</th>\n",
       "      <th>rec 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9515</td>\n",
       "      <td>9518</td>\n",
       "      <td>9517.0</td>\n",
       "      <td>56439.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9517</td>\n",
       "      <td>9515</td>\n",
       "      <td>9518.0</td>\n",
       "      <td>56439.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9518</td>\n",
       "      <td>9515</td>\n",
       "      <td>36027.0</td>\n",
       "      <td>9517.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9521</td>\n",
       "      <td>9537</td>\n",
       "      <td>9541.0</td>\n",
       "      <td>9535.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9523</td>\n",
       "      <td>36027</td>\n",
       "      <td>45806.0</td>\n",
       "      <td>52082.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>66597</td>\n",
       "      <td>52979</td>\n",
       "      <td>64952.0</td>\n",
       "      <td>20642.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>66601</td>\n",
       "      <td>65142</td>\n",
       "      <td>54895.0</td>\n",
       "      <td>54830.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>66603</td>\n",
       "      <td>54895</td>\n",
       "      <td>46571.0</td>\n",
       "      <td>65147.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>66607</td>\n",
       "      <td>54830</td>\n",
       "      <td>65136.0</td>\n",
       "      <td>52979.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>66971</td>\n",
       "      <td>63416</td>\n",
       "      <td>63413.0</td>\n",
       "      <td>63415.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>178 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sku  rec 1    rec 2    rec 3\n",
       "0     9515   9518   9517.0  56439.0\n",
       "1     9517   9515   9518.0  56439.0\n",
       "2     9518   9515  36027.0   9517.0\n",
       "3     9521   9537   9541.0   9535.0\n",
       "4     9523  36027  45806.0  52082.0\n",
       "..     ...    ...      ...      ...\n",
       "173  66597  52979  64952.0  20642.0\n",
       "174  66601  65142  54895.0  54830.0\n",
       "175  66603  54895  46571.0  65147.0\n",
       "176  66607  54830  65136.0  52979.0\n",
       "177  66971  63416  63413.0  63415.0\n",
       "\n",
       "[178 rows x 4 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_upsell_reco(pairs_df_top_neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SKU_2\n",
       "3    123\n",
       "1     29\n",
       "2     26\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs_df_top_neighbors.groupby(\"SKU_1\").agg(\n",
    "    {\"SKU_2\" : \"count\"}\n",
    ").reset_index().value_counts(\"SKU_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neighbors_degree_2 = neighbors_degree_2[neighbors_degree_2[\"TRANSACTION_COUNT\"]>0]\n",
    "# neighbors_degree_2[(neighbors_degree_2[\"SKU_1\"]==9518) & (neighbors_degree_2[\"SKU_2\"].isin(node_neighbor_n_plus_one[9518]))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- # Graph plot -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Network(notebook=True, filter_menu=True, cdn_resources='remote')\n",
    "for edge, data in G.edges.items():\n",
    "    data[\"label\"] = str(round(data[\"weight\"], 2))\n",
    "    \n",
    "net.from_nx(G)\n",
    "net.show_buttons(filter_=['physics'])\n",
    "net.show_buttons()\n",
    "net.show(\"teams.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
